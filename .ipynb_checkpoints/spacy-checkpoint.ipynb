{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e621696-a848-42b5-9a5f-90075ee2dd7c",
   "metadata": {},
   "source": [
    "# Spacy Implementation\n",
    "Adapted from [these](https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/) [tutorials](https://realpython.com/natural-language-processing-spacy-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6424e0f-f337-4757-84a5-353c6a01988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import contractions\n",
    "import re\n",
    "import spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea65250b-c4ac-4c36-a52e-7e4a8f93e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in data\n",
    "df_final = pd.read_csv(\"./data/routes.csv\")\n",
    "\n",
    "#load in model - don't need the disabled pipelines\n",
    "nlp = spacy.load('en_core_web_lg',disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8318c18a-616c-49f6-9736-24751d28fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the nlp features into one all-lowercase column, expand contractions,  remove non-letters\n",
    "df_final['text_combined'] = df_final.apply(lambda x: \" \".join([str(x['description']), \n",
    "                                                           str(x['location']), \n",
    "                                                           str(x['protection'])]).lower(), \n",
    "                                       axis=1)\n",
    "#drop np.nans and remove str np.nans\n",
    "df_final = df_final.dropna(subset='text_combined')\n",
    "df_final['text_combined'] = df_final['text_combined'].apply(lambda x: x.replace(\"np.nan\", \"\"))\n",
    "\n",
    "#expand contractions, remove non-letters\n",
    "df_final['text_combined'] = df_final['text_combined'].map(contractions.fix)\n",
    "df_final['text_combined'] = df_final['text_combined'].apply(lambda x: x.replace(\"'s \", \" \"))\n",
    "df_final['text_combined'] = df_final['text_combined'].apply(lambda x: re.sub(r'[^a-z\\s]', ' ', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d4e4c163-b2f3-445b-93dd-d98207474c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization with stopword and whitespace removal\n",
    "df_final['lemmatized_text_combined'] = df_final['text_combined'].apply(lambda x: ' '.join([token.lemma_ for token in nlp(x) if not token.is_stop and not token.is_space]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a7d8629-080e-4a2c-9a3f-917ad6e2f0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.loc[0, 'lemmatized_text_combined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b93c82a-48e3-4ffc-b6d7-c0c488faf83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find poor pre-processing and resolve\n",
    "def oov_tokens(string):\n",
    "    \"\"\"\n",
    "    Takes in the lemmatized version of the combined text and returns all the words in that text that are out-of-vocabulary\n",
    "    \"\"\"\n",
    "    return \" \".join([str(token) for token in nlp(string) if token.is_oov])\n",
    "\n",
    "\n",
    "\n",
    "df_final['oov'] = df_final['lemmatized_text_combined'].apply(oov_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7daefb2f-2461-4ce5-9f03-89428ad7f3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              83788\n",
       "bouldery       1750\n",
       "lieback         963\n",
       "undercling      900\n",
       "balancy         813\n",
       "sidepull        802\n",
       "cruxy           763\n",
       "undercle        647\n",
       "farth           519\n",
       "chockstone      483\n",
       "tcus            448\n",
       "handcrack       434\n",
       "tricam          428\n",
       "incut           356\n",
       "topout          231\n",
       "permadraw       215\n",
       "bouldere        178\n",
       "balancey        172\n",
       "coldshut        166\n",
       "laybacke        146\n",
       "Name: oov, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final['oov'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4ffbe2-e877-4216-843c-beecca7f5d08",
   "metadata": {},
   "source": [
    "farth = farthest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05caa222-cb10-4d50-8e0e-b04b8310757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordclouds - https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/\n",
    "# Importing wordcloud for plotting word clouds and textwrap for wrapping longer text\n",
    "from wordcloud import WordCloud\n",
    "from textwrap import wrap\n",
    "\n",
    "# Function for generating word clouds\n",
    "def generate_wordcloud(data,title):\n",
    "  wc = WordCloud(width=400, height=330, max_words=150,colormap=\"Dark2\").generate_from_frequencies(data)\n",
    "  plt.figure(figsize=(10,8))\n",
    "  plt.imshow(wc, interpolation='bilinear')\n",
    "  plt.axis(\"off\")\n",
    "  plt.title('\\n'.join(wrap(title,60)),fontsize=13)\n",
    "  plt.show()\n",
    "  \n",
    "# Transposing document term matrix\n",
    "df_dtm=df_dtm.transpose()\n",
    "\n",
    "# Plotting word cloud for each product\n",
    "for index,product in enumerate(df_dtm.columns):\n",
    "  generate_wordcloud(df_dtm[product].sort_values(ascending=False),product)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
