{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ede5deff-19f4-4bb8-85be-812cff37b066",
   "metadata": {},
   "source": [
    "# Transfer Learning Using BERT\n",
    "Adapted from [this tutorial](https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43d602f-6895-4208-81da-26f6b3210f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[label] token token token (up to 512) [sep] (to mark end of doc)\n",
    "\n",
    "#BertTokenizer to tokenize\n",
    "\n",
    "#[PAD] for empty tokens (so each sequence has 512 tokens, including [label], [SEP], and [PAD]s )\n",
    "#this can be a dataframe with two columns, one for the label and one for the text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5b948f-859f-490c-a1ae-effaa1489aad",
   "metadata": {},
   "source": [
    "Using the initial spaCy lemmatized and tokenized dataset, with one 50% dropout, one linear, and a final classification layer, I acheived max validation and testing accuracy of 33% after 5 epochs. Run again with 10 epochs, we started to see divergence between training and validation accuracy after just a few epochs, but test accuracy was 35%, compared to validation accuracy of 33%.\n",
    "\n",
    "### insert image\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
