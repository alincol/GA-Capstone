{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e621696-a848-42b5-9a5f-90075ee2dd7c",
   "metadata": {},
   "source": [
    "# Spacy Implementation\n",
    "Adapted from [these](https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/) [tutorials](https://realpython.com/natural-language-processing-spacy-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a6424e0f-f337-4757-84a5-353c6a01988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import contractions\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4b3005-05d7-4461-8fdd-35b8850c15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/6116978/how-to-replace-multiple-substrings-of-a-string\n",
    "def multiple_replace(string, rep_dict):\n",
    "    pattern = re.compile(\"|\".join([re.escape(k) for k in sorted(rep_dict,key=len,reverse=True)]), flags=re.DOTALL)\n",
    "    return pattern.sub(lambda x: rep_dict[x.group(0)], string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea65250b-c4ac-4c36-a52e-7e4a8f93e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in data - this has already been pre-processed according to the 1-data-collection notebook, they need to be combined\n",
    "df_final = pd.read_csv(\"./data/routes.csv\")\n",
    "\n",
    "#load in model - don't need the disabled pipelines\n",
    "nlp = spacy.load('en_core_web_lg',disable=['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "446a3c91-05b2-4db1-8e18-c3a41a2222f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route_name</th>\n",
       "      <th>safety</th>\n",
       "      <th>fa</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>protection</th>\n",
       "      <th>grade.YDS</th>\n",
       "      <th>type.trad</th>\n",
       "      <th>metadata.parent_lnglat</th>\n",
       "      <th>metadata.parent_sector</th>\n",
       "      <th>...</th>\n",
       "      <th>plus</th>\n",
       "      <th>minus</th>\n",
       "      <th>plus_minus</th>\n",
       "      <th>grade_numeric_plus_minus</th>\n",
       "      <th>year_established</th>\n",
       "      <th>stratify</th>\n",
       "      <th>grade_reduced</th>\n",
       "      <th>text_combined</th>\n",
       "      <th>lemmatized_text_combined</th>\n",
       "      <th>oov</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [route_name, safety, fa, description, location, protection, grade.YDS, type.trad, metadata.parent_lnglat, metadata.parent_sector, metadata.mp_route_id, metadata.mp_sector_id, metadata.mp_path, type.sport, type.tr, type.alpine, type.snow, type.mixed, type.ice, grade_numeric, plus, minus, plus_minus, grade_numeric_plus_minus, year_established, stratify, grade_reduced, text_combined, lemmatized_text_combined, oov]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 30 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[df_final['lemmatized_text_combined'].str.contains('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8318c18a-616c-49f6-9736-24751d28fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the nlp features into one all-lowercase column, expand contractions,  remove non-letters\n",
    "df_final['text_combined'] = df_final.apply(lambda x: \" \".join([str(x['description']), \n",
    "                                                           str(x['location']), \n",
    "                                                           str(x['protection'])]).lower(), \n",
    "                                       axis=1)\n",
    "#drop np.nans and remove str np.nans\n",
    "df_final = df_final.dropna(subset='text_combined')\n",
    "df_final['text_combined'] = df_final['text_combined'].apply(lambda x: x.replace(\"np.nan\", \"\"))\n",
    "\n",
    "#expand contractions, remove non-letters\n",
    "df_final['text_combined'] = df_final['text_combined'].map(contractions.fix)\n",
    "df_final['text_combined'] = df_final['text_combined'].apply(lambda x: x.replace(\"'s \", \" \"))\n",
    "df_final['text_combined'] = df_final['text_combined'].apply(lambda x: re.sub(r'[^a-z\\s]', ' ', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d4e4c163-b2f3-445b-93dd-d98207474c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization with stopword and whitespace removal\n",
    "df_final['lemmatized_text_combined'] = df_final['text_combined'].apply(lambda x: ' '.join([token.lemma_ for token in nlp(x) if not token.is_stop and not token.is_space]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7a7d8629-080e-4a2c-9a3f-917ad6e2f0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         go slab bolt steep corner gear leave corner st...\n",
       "1         fun move break scree fill ledge big bush crux ...\n",
       "2         pretty cool orange arete sport interesting cli...\n",
       "3         climb open look slab eastern aspect wall close...\n",
       "4         good route wall crux move crack thin seam time...\n",
       "                                ...                        \n",
       "127508    brush steep slab low corner west face horizont...\n",
       "127509    friction slab past bolt gear bulge bolt easy r...\n",
       "127510    face climb right mom meatfloaf step gully low ...\n",
       "127511    start leave mom meatloaf climb bolt face cross...\n",
       "127512    begin distinct layback crack split low angle e...\n",
       "Name: lemmatized_text_combined, Length: 127504, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final['lemmatized_text_combined']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4ffbe2-e877-4216-843c-beecca7f5d08",
   "metadata": {},
   "source": [
    "farth = farthest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c3997273-3d58-406e-b8d9-bc80da470049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127513, 29)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "02715c64-7e64-43a6-929f-dc3dd1550a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "nans_to_drop = df_final[(df_final['lemmatized_text_combined'] == '') | (df_final['lemmatized_text_combined'] == 'nan')].index\n",
    "df_final = df_final.drop(nans_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "51df95dd-fcb6-4d72-a062-dbb0ada35204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((127504, 29), (91802, 29), (22951, 29), (12751, 29))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(df_final, test_size=.1, random_state=42, stratify=df_final['stratify'])\n",
    "train, validation = train_test_split(train, test_size=.2, random_state=42, stratify=train['stratify'])\n",
    "\n",
    "df_final.shape, train.shape, validation.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "81d4d7aa-7bac-4933-81cf-389c9bf5eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv\n",
    "df_final.to_csv(\"./data/spacy_routes.csv\", index=False)\n",
    "train.to_csv(\"./data/spacy_train.csv\", index=False)\n",
    "validation.to_csv(\"./data/spacy_val.csv\", index=False)\n",
    "test.to_csv(\"./data/spacy_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0b93c82a-48e3-4ffc-b6d7-c0c488faf83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find poor pre-processing and resolve\n",
    "def oov_tokens(string):\n",
    "    \"\"\"\n",
    "    Takes in the lemmatized version of the combined text and returns all the words in that text that are out-of-vocabulary\n",
    "    \"\"\"\n",
    "    return \" \".join([str(token) for token in nlp(string) if token.is_oov])\n",
    "\n",
    "\n",
    "\n",
    "df_final['oov'] = df_final['lemmatized_text_combined'].apply(oov_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7daefb2f-2461-4ce5-9f03-89428ad7f3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              83779\n",
       "bouldery       1750\n",
       "lieback         963\n",
       "undercling      900\n",
       "balancy         813\n",
       "sidepull        802\n",
       "cruxy           763\n",
       "undercle        647\n",
       "farth           519\n",
       "chockstone      483\n",
       "tcus            448\n",
       "handcrack       434\n",
       "tricam          428\n",
       "incut           356\n",
       "topout          231\n",
       "permadraw       215\n",
       "bouldere        178\n",
       "balancey        172\n",
       "coldshut        166\n",
       "laybacke        146\n",
       "Name: oov, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final['oov'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2974a0d-3731-4e31-b562-975ba995a8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_replace = {\n",
    "    \"balancey\" : 'balancy',\n",
    "    'bouldere' : 'bouldery',\n",
    "    'laybacke' : 'lieback',\n",
    "    'layback' : 'lieback',\n",
    "    'undercle' : 'undercling',\n",
    "    \n",
    "}\n",
    "\n",
    "df_final['lemmatized_text_combined_cleaned'] = df_final['lemmatized_text_combined'].apply(lambda x: x.replace())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05caa222-cb10-4d50-8e0e-b04b8310757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordclouds - https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/\n",
    "# Importing wordcloud for plotting word clouds and textwrap for wrapping longer text\n",
    "from wordcloud import WordCloud\n",
    "from textwrap import wrap\n",
    "\n",
    "# Function for generating word clouds\n",
    "def generate_wordcloud(data,title):\n",
    "  wc = WordCloud(width=400, height=330, max_words=150,colormap=\"Dark2\").generate_from_frequencies(data)\n",
    "  plt.figure(figsize=(10,8))\n",
    "  plt.imshow(wc, interpolation='bilinear')\n",
    "  plt.axis(\"off\")\n",
    "  plt.title('\\n'.join(wrap(title,60)),fontsize=13)\n",
    "  plt.show()\n",
    "  \n",
    "# Transposing document term matrix\n",
    "df_dtm=df_dtm.transpose()\n",
    "\n",
    "# Plotting word cloud for each product\n",
    "for index,product in enumerate(df_dtm.columns):\n",
    "  generate_wordcloud(df_dtm[product].sort_values(ascending=False),product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea82c3c7-6919-41b4-be2d-0e28ff1a83b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
